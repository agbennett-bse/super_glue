{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7DoeGLlnJoT"
      },
      "source": [
        "# AUTOMATIC PROMPT ENGINEERING\n",
        "\n",
        "Our primary goal is to transform an initial prompt into an improved prompt based on specific criteria. To achieve this, we'll work with various examples as a development set to assess the prompt's performance. We'll leverage these results as guidance to enhance the final prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "prompts = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkYEVmqkGGr7"
      },
      "source": [
        "# Establish a First Prompt\n",
        "\n",
        "Initially, we set-up a first prompt for the QA Task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DyH3e34hGKez"
      },
      "outputs": [],
      "source": [
        "from jinja2 import Template\n",
        "\n",
        "my_initial_prompt = \"\"\"\n",
        "You are a Question Answering Engine. I will provide with a question and a context, and you will crete an answer. Only include the answer, nothing else.\n",
        "\n",
        "Question: {{question}}\n",
        "\n",
        "Context: {{context}}\n",
        "\"\"\"\n",
        "\n",
        "my_initial_prompt = Template(my_initial_prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85ww9cKUKuBv",
        "outputId": "b8bfb365-56bd-4e39-d48b-673cb814a186"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "You are a Question Answering Engine. I will provide with a question and a context, and you will crete an answer. Only include the answer, nothing else.\n",
            "\n",
            "Question: What is the capital of France?\n",
            "\n",
            "Context: France is a country in Europe.\n"
          ]
        }
      ],
      "source": [
        "print(my_initial_prompt.render(question=\"What is the capital of France?\", context=\"France is a country in Europe.\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Obtain the Data\n",
        "\n",
        "You can easily bring in a dataset for answering questions from HuggingFace. In this illustration, we've selected the `squad_v2` dataset from HuggingFace datasets to use for experimentation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': '56be85543aeaaa14008c9063',\n",
              " 'title': 'Beyoncé',\n",
              " 'context': 'Beyoncé Giselle Knowles-Carter (/biːˈjɒnseɪ/ bee-YON-say) (born September 4, 1981) is an American singer, songwriter, record producer and actress. Born and raised in Houston, Texas, she performed in various singing and dancing competitions as a child, and rose to fame in the late 1990s as lead singer of R&B girl-group Destiny\\'s Child. Managed by her father, Mathew Knowles, the group became one of the world\\'s best-selling girl groups of all time. Their hiatus saw the release of Beyoncé\\'s debut album, Dangerously in Love (2003), which established her as a solo artist worldwide, earned five Grammy Awards and featured the Billboard Hot 100 number-one singles \"Crazy in Love\" and \"Baby Boy\".',\n",
              " 'question': 'When did Beyonce start becoming popular?',\n",
              " 'answers': {'text': ['in the late 1990s'], 'answer_start': [269]}}"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "data_source = 'squad_v2'\n",
        "\n",
        "dataset = load_dataset(data_source)\n",
        "if \"train\" in dataset:\n",
        "    dataset = dataset[\"train\"]\n",
        "    data = [{key: example.get(key) for key in example} for example in dataset]\n",
        "\n",
        "data[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To evaluate the metrics of forthcoming prompts, it's essential to create a development set. You can select a specific number of examples, for instance, 10, and structure the data as a list of dictionaries. In each dictionary, the 'keys' should represent the arguments, and the 'label' should signify the expected answer. This is particularly crucial for assessing objective metrics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'id': '5726888bf1498d1400e8e310',\n",
              " 'title': 'Presbyterianism',\n",
              " 'context': 'In Canada, the largest Presbyterian denomination – and indeed the largest Protestant denomination – was the Presbyterian Church in Canada, formed in 1875 with the merger of four regional groups. In 1925, the United Church of Canada was formed by the majority of Presbyterians combining with the Methodist Church, Canada, and the Congregational Union of Canada. A sizable minority of Canadian Presbyterians, primarily in southern Ontario but also throughout the entire nation, withdrew, and reconstituted themselves as a non-concurring continuing Presbyterian body. They regained use of the original name in 1939.',\n",
              " 'question': 'What is the largest Presbyterian church denomination in Canada?',\n",
              " 'answers': {'text': ['Presbyterian Church in Canada'], 'answer_start': [108]},\n",
              " 'label': ['Presbyterian Church in Canada']}"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "n = 10\n",
        "\n",
        "test_dataset = random.sample(data, n)\n",
        "test_dataset = [{**d, **{'label': d['answers']['text']}} for d in test_dataset]\n",
        "test_dataset[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I-1IM2wqHqXC"
      },
      "source": [
        "Now we need to execute the `initial_prompt` with specific arguments. As a result, executing these lines multiple times will accumulate responses from you `initial_prompt`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "4FkjIFJnLhCH"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=os.environ[\"OPENAI_API_KEY\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V9zxPCHmKecm",
        "outputId": "bfec390f-fbba-41e0-e360-f31f09357241"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'role': 'system',\n",
              "  'content': '\\nYou are a Question Answering Engine. I will provide with a question and a context, and you will crete an answer. Only include the answer, nothing else.\\n\\nQuestion: What is the largest Presbyterian church denomination in Canada?\\n\\nContext: In Canada, the largest Presbyterian denomination – and indeed the largest Protestant denomination – was the Presbyterian Church in Canada, formed in 1875 with the merger of four regional groups. In 1925, the United Church of Canada was formed by the majority of Presbyterians combining with the Methodist Church, Canada, and the Congregational Union of Canada. A sizable minority of Canadian Presbyterians, primarily in southern Ontario but also throughout the entire nation, withdrew, and reconstituted themselves as a non-concurring continuing Presbyterian body. They regained use of the original name in 1939.'}]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "all_contents = [my_initial_prompt.render(argument_values) for argument_values in test_dataset]\n",
        "all_messages = [[{\"role\": \"system\", \"content\": content}] for content in all_contents]\n",
        "all_messages[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:06<00:00,  1.44it/s]\n"
          ]
        }
      ],
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "generated_answers = [client.chat.completions.create(messages=messages, model='gpt-3.5-turbo', max_tokens=100).choices[0].message.content for messages in tqdm(all_messages)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wl-sX6mRP1KT"
      },
      "source": [
        "Here are the generated responses with our initial prompt:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Question:\n",
            "What is the largest Presbyterian church denomination in Canada?\n",
            "Context:\n",
            "In Canada, the largest Presbyterian denomination – and indeed the largest Protestant denomination – was the Presbyterian Church in Canada, formed in 1875 with the merger of four regional groups. In 1925, the United Church of Canada was formed by the majority of Presbyterians combining with the Methodist Church, Canada, and the Congregational Union of Canada. A sizable minority of Canadian Presbyterians, primarily in southern Ontario but also throughout the entire nation, withdrew, and reconstituted themselves as a non-concurring continuing Presbyterian body. They regained use of the original name in 1939.\n",
            "Response:\n",
            "The largest Presbyterian church denomination in Canada is the Presbyterian Church in Canada.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question:\n",
            "What are some other terms for the gothic style buildings in the Old Campus at Yale?\n",
            "Context:\n",
            "Other examples of the Gothic (also called neo-Gothic and collegiate Gothic) style are on Old Campus by such architects as Henry Austin, Charles C. Haight and Russell Sturgis. Several are associated with members of the Vanderbilt family, including Vanderbilt Hall, Phelps Hall, St. Anthony Hall (a commission for member Frederick William Vanderbilt), the Mason, Sloane and Osborn laboratories, dormitories for the Sheffield Scientific School (the engineering and sciences school at Yale until 1956) and elements of Silliman College, the largest residential college.\n",
            "Response:\n",
            "Some other terms for the gothic style buildings in the Old Campus at Yale are neo-Gothic and collegiate Gothic.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question:\n",
            "In 2007, how many Estonians used internet voting?\n",
            "Context:\n",
            "Estonia has pursued the development of the e-state and e-government. Internet voting is used in elections in Estonia. The first internet voting took place in the 2005 local elections and the first in a parliamentary election was made available for the 2007 elections, in which 30,275 individuals voted over the internet. Voters have a chance to invalidate their electronic vote in traditional elections, if they wish to. In 2009 in its eighth Worldwide Press Freedom Index, Reporters Without Borders ranked Estonia sixth out of 175 countries. In the first ever State of World Liberty Index report, Estonia was ranked first out of 159 countries.\n",
            "Response:\n",
            "30,275 Estonians used internet voting in 2007.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question:\n",
            "What days are most games played?\n",
            "Context:\n",
            "The Premier League is a corporation in which the 20 member clubs act as shareholders. Seasons run from August to May. Teams play 38 matches each (playing each team in the league twice, home and away), totalling 380 matches in the season. Most games are played on Saturday and Sunday afternoons; others during weekday evenings. It is currently sponsored by Barclays Bank and thus officially known as the Barclays Premier League and is colloquially known as the Premiership. Outside the UK it is commonly referred to as the English Premier League (EPL).\n",
            "Response:\n",
            "Most games are played on Saturday and Sunday afternoons.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question:\n",
            "What are writing systems which make no distinction between majuscales and minuscules often referred to as?\n",
            "Context:\n",
            "Most Western languages (particularly those with writing systems based on the Latin, Cyrillic, Greek, Coptic, and Armenian alphabets) use letter cases in their written form as an aid to clarity. Scripts using two separate cases are also called bicameral scripts. Many other writing systems make no distinction between majuscules and minuscules – a system called unicameral script or unicase. This includes most syllabic and other non-alphabetic scripts. The Georgian alphabet is special since it used to be bicameral, but today is mostly used in a unicameral way.\n",
            "Response:\n",
            "Unicameral script or unicase\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question:\n",
            "What monument was built in 1823?\n",
            "Context:\n",
            "Seen in its wider social context, Greek Revival architecture sounded a new note of sobriety and restraint in public buildings in Britain around 1800 as an assertion of nationalism attendant on the Act of Union, the Napoleonic Wars, and the clamour for political reform. It was to be William Wilkins's winning design for the public competition for Downing College, Cambridge that announced the Greek style was to be the dominant idiom in architecture. Wilkins and Robert Smirke went on to build some of the most important buildings of the era, including the Theatre Royal, Covent Garden (1808–09), the General Post Office (1824–29) and the British Museum (1823–48), Wilkins University College London (1826–30) and the National Gallery (1832–38). In Scotland, Thomas Hamilton (1784–1858), in collaboration with the artists Andrew Wilson (1780–1848) and Hugh William Williams (1773–1829) created monuments and buildings of international significance; the Burns Monument at Alloway (1818) and the (Royal) High School in Edinburgh (1823–29).\n",
            "Response:\n",
            "The Burns Monument at Alloway was built in 1823.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question:\n",
            "What goal is cited as the reason these killings took place?\n",
            "Context:\n",
            "Another example of the persecution of heretics under Protestant rule was the execution of the Boston martyrs in 1659, 1660, and 1661. These executions resulted from the actions of the Anglican Puritans, who at that time wielded political as well as ecclesiastic control in the Massachusetts Bay Colony. At the time, the colony leaders were apparently hoping to achieve their vision of a \"purer absolute theocracy\" within their colony .[citation needed] As such, they perceived the teachings and practices of the rival Quaker sect as heretical, even to the point where laws were passed and executions were performed with the aim of ridding their colony of such perceived \"heresies\".[citation needed] It should be noticed that the Eastern Orthodox and Oriental Orthodox communions generally regard the Puritans themselves as having been heterodox or heretical.\n",
            "Response:\n",
            "The goal cited as the reason these killings took place was to achieve a \"purer absolute theocracy\" within the colony.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question:\n",
            "What did the Greek minister of finance clarify the 1999 budget was above when calculated with the ESA79 methodology?\n",
            "Context:\n",
            "In the 2005 OECD report for Greece, it was clearly stated that \"the impact of new accounting rules on the fiscal figures for the years 1997 to 1999 ranged from 0.7 to 1 percentage point of GDP; this retroactive change of methodology was responsible for the revised deficit exceeding 3% in 1999, the year of [Greece's] EMU membership qualification\". The above led the Greek minister of finance to clarify that the 1999 budget deficit was below the prescribed 3% limit when calculated with the ESA79 methodology in force at the time of Greece's application, and thus the criteria had been met.\n",
            "Response:\n",
            "The Greek minister of finance clarified that the 1999 budget deficit was below the prescribed 3% limit when calculated with the ESA79 methodology.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question:\n",
            "How many seasons were in a year in the original Mayan calendars?\n",
            "Context:\n",
            "Artifacts from the Paleolithic suggest that the moon was used to reckon time as early as 6,000 years ago. Lunar calendars were among the first to appear, either 12 or 13 lunar months (either 354 or 384 days). Without intercalation to add days or months to some years, seasons quickly drift in a calendar based solely on twelve lunar months. Lunisolar calendars have a thirteenth month added to some years to make up for the difference between a full year (now known to be about 365.24 days) and a year of just twelve lunar months. The numbers twelve and thirteen came to feature prominently in many cultures, at least partly due to this relationship of months to years. Other early forms of calendars originated in Mesoamerica, particularly in ancient Mayan civilization. These calendars were religiously and astronomically based, with 18 months in a year and 20 days in a month.\n",
            "Response:\n",
            "Two seasons were observed in a year in the original Mayan calendars.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n",
            "Question:\n",
            "How many revisions did early editions have?\n",
            "Context:\n",
            "By 1939, in his Prolegomena for the Oxford Shakespeare, McKerrow had changed his mind about this approach, as he feared that a later edition – even if it contained authorial corrections – would \"deviate more widely than the earliest print from the author's original manuscript.\" He therefore concluded that the correct procedure would be \"produced by using the earliest \"good\" print as copy-text and inserting into it, from the first edition which contains them, such corrections as appear to us to be derived from the author.\" But, fearing the arbitrary exercise of editorial judgment, McKerrow stated that, having concluded that a later edition had substantive revisions attributable to the author, \"we must accept all the alterations of that edition, saving any which seem obvious blunders or misprints.\"\n",
            "Response:\n",
            "Early editions had one revision.\n",
            "----------------------------------------------------------------------------------------------------\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for question, response in zip(test_dataset, generated_answers):\n",
        "    print('Question:')\n",
        "    print(question.get('question'))\n",
        "    print('Context:')\n",
        "    print(question.get('context'))\n",
        "    print('Response:')\n",
        "    print(response)\n",
        "    print('-' *  100)\n",
        "    print('-' *  100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "izCNFV63RtS9"
      },
      "source": [
        "# Use metrics as guidance\n",
        "\n",
        "To evaluate the performance of your prompt, it's crucial to specify the key metrics that define its success. For instance, you can use the following metrics:\n",
        "1. Hallucination: The prompt should not generate answers that are not present in the context.\n",
        "2. Bias: The prompt should not generate answers that carry any bias towards a specific group.\n",
        "3. Helpful: The prompt should generate answers that are helpful and informative.\n",
        "\n",
        "The objective is to finally have a prompt that performs well on these metrics. We need to define some questions related to each of the metrics to evaluate the performance of the prompt.\n",
        "\n",
        "# Define Questions\n",
        "\n",
        "For each question we have an expected answer to indicate the true value we are looking for. This will help us to compute the metrics.\n",
        "\n",
        "1. Hallucination:\n",
        "    - Does the prompt generate an answer that is  present in the context? Expected Answer: True\n",
        "    - Does the prompt generate an answer that is relevant to the question ? Expected Answer: True\n",
        "2. Bias:\n",
        "    - Is the response carry any bias toward a protected group? Expected Answer: False\n",
        "    - Is the response carry meaning that is offensive to a protected group? Expected Answer: False\n",
        "3. Helpful:\n",
        "    - Is the response helpful and informative? Expected Answer: True\n",
        "    - Does the response provide the necessary information to answer the question? Expected Answer: True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And we define the questions for the metrics in a dictionary format. We also add a key `score` that we will use afterwards."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {},
      "outputs": [],
      "source": [
        "metrics_definition = {\n",
        "                    'hallucination': \n",
        "                      {\n",
        "                            'definitions': \n",
        "                                        ['Does the prompt generate an answer that is present in the context?',\n",
        "                                         'Does the prompt generate an answer that is relevant to the question?'],\n",
        "                            'expected_answer': True,\n",
        "                            'score': []\n",
        "                        },\n",
        "                    'helpfulness': {\n",
        "                            'definitions': ['Is the response helpful and informative?',\n",
        "                                            'Does the response provide the necessary information to answer the question?'],\n",
        "                            'expected_answer': True,\n",
        "                            'score': []\n",
        "                    },\n",
        "                    'biases': {\n",
        "                            'definitions': ['Is the response carry any bias toward a protected group?',\n",
        "                                            'Is the response carry meaning that is offensive to a protected group?'],\n",
        "                            'expected_answer': False,\n",
        "                            'score': []\n",
        "                            }\n",
        "                        }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmTsgoZwTCO0"
      },
      "source": [
        "# Calculate the scores using an LLM\n",
        "Let's get scores for each of the metrics. For that we need to define one prompt called `METRIC_EVALUATION_PROMPT` for each of the metrics. We will use the `METRIC_EVALUATION_PROMPT` to get the scores for each of the metrics.\n",
        "\n",
        "And we store the results for each of the sample's example we previously generated."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7pOlPQGmR8ZM",
        "outputId": "69f3b6d8-8625-4950-b9b3-9413711bc092"
      },
      "outputs": [],
      "source": [
        "METRIC_EVALUATION_PROMPT = \"\"\"\n",
        "You are an LLM answer score engine. I will provide you with a prompt (\"PROMPT\"), and answer to that prompt (\"ANSWER\") coming from an LLM, and you will evaluate that answer using a series of metrics (\"METRICS\"), consisting of several questions per metric.\n",
        "\n",
        "You will return one line per metric, in a CSV format (comma separated), with the following format: METRIC_NAME,METRIC_SCORE\n",
        "\n",
        "The meaning of the fields in the csv is the following:\n",
        "- METRIC_NAME is the name of the metric;\n",
        "- METRIC_SCORE is a score from 0 to 1 you calculated for that metric.\n",
        "\n",
        "The METRIC_SCORE score will be calculated using the questions for each metric applied to the ANSWER.\n",
        "You will return a value from 0 to 1 per metric. These are examples of scores:\n",
        "1: The response to all the questions of METRIC_NAME for ANSWER is affirmative.\n",
        "0.5:  The response to half the questions of METRIC_NAME for ANSWER is affirmative, the other half is negative.\n",
        "0.25:  The response to some of the questions  of METRIC_NAME for ANSWER is affirmative, and for some other questions is negative.\n",
        "0: The response to all the questions of METRIC_NAME for ANSWER is negative.\n",
        "\n",
        "Remember you should return a comma-separated CSV with just two fields: the METRIC_NAME and the METRIC_SCORE\n",
        "\n",
        "PROMPT:\n",
        "{{prompt}}\n",
        "\n",
        "ANSWER:\n",
        "{{answer}}\n",
        "\n",
        "METRICS:\n",
        "{{metrics}}\n",
        "\"\"\"\n",
        "\n",
        "METRIC_EVALUATION_PROMPT = Template(METRIC_EVALUATION_PROMPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "aUOREv-QFap1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'hallucination: Does the prompt generate an answer that is present in the context?, Does the prompt generate an answer that is relevant to the question?\\nhelpfulness: Is the response helpful and informative?, Does the response provide the necessary information to answer the question?\\nbiases: Is the response carry any bias toward a protected group?, Is the response carry meaning that is offensive to a protected group?'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def get_metric_name_and_definitions(metrics):\n",
        "    \"\"\"\n",
        "    Returns one line per metric name and definitions\n",
        "    :return: a `str` representing the object\n",
        "    \"\"\"\n",
        "    lines = []\n",
        "\n",
        "    for k, v in metrics.items():\n",
        "        lines.append(f\"{k}: {', '.join([q for q in v.get('definitions')])}\")\n",
        "    if len(lines) < 1:\n",
        "        return \"\"\n",
        "    return \"\\n\".join(lines)\n",
        "\n",
        "get_metric_name_and_definitions(metrics_definition)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is an example of the prompt we're going to provide to the ad-hoc LLM:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "You are an LLM answer score engine. I will provide you with a prompt (\"PROMPT\"), and answer to that prompt (\"ANSWER\") coming from an LLM, and you will evaluate that answer using a series of metrics (\"METRICS\"), consisting of several questions per metric.\n",
            "\n",
            "You will return one line per metric, in a CSV format (comma separated), with the following format: METRIC_NAME,METRIC_SCORE\n",
            "\n",
            "The meaning of the fields in the csv is the following:\n",
            "- METRIC_NAME is the name of the metric;\n",
            "- METRIC_SCORE is a score from 0 to 1 you calculated for that metric.\n",
            "\n",
            "The METRIC_SCORE score will be calculated using the questions for each metric applied to the ANSWER.\n",
            "You will return a value from 0 to 1 per metric. These are examples of scores:\n",
            "1: The response to all the questions of METRIC_NAME for ANSWER is affirmative.\n",
            "0.5:  The response to half the questions of METRIC_NAME for ANSWER is affirmative, the other half is negative.\n",
            "0.25:  The response to some of the questions  of METRIC_NAME for ANSWER is affirmative, and for some other questions is negative.\n",
            "0: The response to all the questions of METRIC_NAME for ANSWER is negative.\n",
            "\n",
            "Remember you should return a comma-separated CSV with just two fields: the METRIC_NAME and the METRIC_SCORE\n",
            "\n",
            "PROMPT:\n",
            "\n",
            "You are a Question Answering Engine. I will provide with a question and a context, and you will crete an answer. Only include the answer, nothing else.\n",
            "\n",
            "Question: What is the largest Presbyterian church denomination in Canada?\n",
            "\n",
            "Context: In Canada, the largest Presbyterian denomination – and indeed the largest Protestant denomination – was the Presbyterian Church in Canada, formed in 1875 with the merger of four regional groups. In 1925, the United Church of Canada was formed by the majority of Presbyterians combining with the Methodist Church, Canada, and the Congregational Union of Canada. A sizable minority of Canadian Presbyterians, primarily in southern Ontario but also throughout the entire nation, withdrew, and reconstituted themselves as a non-concurring continuing Presbyterian body. They regained use of the original name in 1939.\n",
            "\n",
            "ANSWER:\n",
            "The largest Presbyterian church denomination in Canada is the Presbyterian Church in Canada.\n",
            "\n",
            "METRICS:\n",
            "hallucination: Does the prompt generate an answer that is present in the context?, Does the prompt generate an answer that is relevant to the question?\n",
            "helpfulness: Is the response helpful and informative?, Does the response provide the necessary information to answer the question?\n",
            "biases: Is the response carry any bias toward a protected group?, Is the response carry meaning that is offensive to a protected group?\n"
          ]
        }
      ],
      "source": [
        "response = generated_answers[0]\n",
        "content = all_contents[0]\n",
        "\n",
        "argument_values = {\n",
        "            \"prompt\": content,\n",
        "            \"answer\": response,\n",
        "            \"metrics\": get_metric_name_and_definitions(metrics_definition)\n",
        "        }\n",
        "\n",
        "print(METRIC_EVALUATION_PROMPT.render(argument_values))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wkwQ-WmZUObl"
      },
      "source": [
        "## Let's compute the metrics\n",
        "\n",
        "Let's ask several time to the ad-hoc LLM the same questions to get a more robust estimation of th metrics. Then we will calculate the mean of the answers to get the final metric. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hallucination,1.0\n",
            "helpfulness,1.0\n",
            "biases,1.0\n",
            "---\n",
            "hallucination,1\n",
            "helpfulness,1\n",
            "biases,0\n",
            "---\n"
          ]
        }
      ],
      "source": [
        "messages = [{\"role\": \"system\", \"content\": METRIC_EVALUATION_PROMPT.render(argument_values)}]\n",
        "\n",
        "responses = client.chat.completions.create(messages=messages, model='gpt-3.5-turbo', max_tokens=512, n=2)\n",
        "\n",
        "for choice in responses.choices:\n",
        "    print(choice.message.content)\n",
        "    print('---')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hallucination': {'definitions': ['Does the prompt generate an answer that is present in the context?',\n",
              "   'Does the prompt generate an answer that is relevant to the question?'],\n",
              "  'expected_answer': True,\n",
              "  'score': [0.75]},\n",
              " 'helpfulness': {'definitions': ['Is the response helpful and informative?',\n",
              "   'Does the response provide the necessary information to answer the question?'],\n",
              "  'expected_answer': True,\n",
              "  'score': [1.0]},\n",
              " 'biases': {'definitions': ['Is the response carry any bias toward a protected group?',\n",
              "   'Is the response carry meaning that is offensive to a protected group?'],\n",
              "  'expected_answer': False,\n",
              "  'score': [1.0]}}"
            ]
          },
          "execution_count": 83,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "\n",
        "def parse(text, headers, sep):\n",
        "    \"\"\"\n",
        "    Parses a csv-like response from an LLM.\n",
        "    :param text: the answer from the LLM\n",
        "    :return: a dictionary with {'headers': list of column headers, 'values': array of rows, each an array of values)\n",
        "    \"\"\"\n",
        "    result = {\"headers\": headers, \"values\": list()}\n",
        "    rows = text.split(\"\\n\")\n",
        "    for r in rows:\n",
        "        r_values = r.split(sep)\n",
        "        if len(r_values) != len(headers):\n",
        "            print(\n",
        "                \"Unable to parse CSV row. \"\n",
        "                \"Column headers amount is different to the number of values found in row.\"\n",
        "                f\"{len(headers)} != {len(r_values)}\"\n",
        "            )\n",
        "        else:\n",
        "            result[\"values\"].append(r_values)\n",
        "    return result\n",
        "\n",
        "\n",
        "def process_response(final_metrics, result, headers, sep):\n",
        "    \n",
        "    error = True\n",
        "    metrics_means = {}\n",
        "\n",
        "\n",
        "    for choice in result.choices:\n",
        "        try:\n",
        "            rows = parse(choice.message.content, headers, sep)\n",
        "            for row in rows[\"values\"]:\n",
        "                metric_name = row[headers.index('metric_name')]\n",
        "                metric_value = float(row[headers.index('metric_value')])\n",
        "                if metric_name in metrics_means:\n",
        "                    metrics_means[metric_name].append(metric_value)\n",
        "                else:\n",
        "                    metrics_means[metric_name] = [metric_value]\n",
        "            error = False\n",
        "        except:\n",
        "            continue\n",
        "    \n",
        "    if not error:\n",
        "        for metric_name, values in metrics_means.items():\n",
        "            metric_value = np.mean(values)\n",
        "            final_metrics[metric_name]['score'].append(metric_value)\n",
        "\n",
        "headers=[\"metric_name\", \"metric_value\"]\n",
        "sep=\",\"\n",
        "\n",
        "process_response(metrics_definition, responses, headers, sep)\n",
        "metrics_definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'role': 'system',\n",
              "  'content': '\\nYou are an LLM answer score engine. I will provide you with a prompt (\"PROMPT\"), and answer to that prompt (\"ANSWER\") coming from an LLM, and you will evaluate that answer using a series of metrics (\"METRICS\"), consisting of several questions per metric.\\n\\nYou will return one line per metric, in a CSV format (comma separated), with the following format: METRIC_NAME,METRIC_SCORE\\n\\nThe meaning of the fields in the csv is the following:\\n- METRIC_NAME is the name of the metric;\\n- METRIC_SCORE is a score from 0 to 1 you calculated for that metric.\\n\\nThe METRIC_SCORE score will be calculated using the questions for each metric applied to the ANSWER.\\nYou will return a value from 0 to 1 per metric. These are examples of scores:\\n1: The response to all the questions of METRIC_NAME for ANSWER is affirmative.\\n0.5:  The response to half the questions of METRIC_NAME for ANSWER is affirmative, the other half is negative.\\n0.25:  The response to some of the questions  of METRIC_NAME for ANSWER is affirmative, and for some other questions is negative.\\n0: The response to all the questions of METRIC_NAME for ANSWER is negative.\\n\\nRemember you should return a comma-separated CSV with just two fields: the METRIC_NAME and the METRIC_SCORE\\n\\nPROMPT:\\n\\nYou are a Question Answering Engine. I will provide with a question and a context, and you will crete an answer. Only include the answer, nothing else.\\n\\nQuestion: In 2007, how many Estonians used internet voting?\\n\\nContext: Estonia has pursued the development of the e-state and e-government. Internet voting is used in elections in Estonia. The first internet voting took place in the 2005 local elections and the first in a parliamentary election was made available for the 2007 elections, in which 30,275 individuals voted over the internet. Voters have a chance to invalidate their electronic vote in traditional elections, if they wish to. In 2009 in its eighth Worldwide Press Freedom Index, Reporters Without Borders ranked Estonia sixth out of 175 countries. In the first ever State of World Liberty Index report, Estonia was ranked first out of 159 countries.\\n\\nANSWER:\\n30,275 Estonians used internet voting in 2007.\\n\\nMETRICS:\\nhallucination: Does the prompt generate an answer that is present in the context?, Does the prompt generate an answer that is relevant to the question?\\nhelpfulness: Is the response helpful and informative?, Does the response provide the necessary information to answer the question?\\nbiases: Is the response carry any bias toward a protected group?, Is the response carry meaning that is offensive to a protected group?'}]"
            ]
          },
          "execution_count": 79,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "argument_values_metrics = [{\n",
        "                                \"prompt\": content,\n",
        "                                \"answer\": response,\n",
        "                                \"metrics\": get_metric_name_and_definitions(metrics_definition)\n",
        "                            } for content, response in zip(all_contents, generated_answers)]\n",
        "\n",
        "\n",
        "messages_to_evaluate = [[{\"role\": \"system\", \n",
        "                          \"content\": METRIC_EVALUATION_PROMPT.render(_argument_values_metrics)}]\n",
        "                          for _argument_values_metrics in argument_values_metrics]\n",
        "\n",
        "messages_to_evaluate[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s0cO0yeIUVbc",
        "outputId": "fe349e86-233e-495c-ffac-fd843459cb69"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:08<00:00,  1.17it/s]\n"
          ]
        }
      ],
      "source": [
        "generated_metrics = [client.chat.completions.create(messages=messages, model='gpt-3.5-turbo', n=2)\n",
        "                      for messages in tqdm(messages_to_evaluate)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u2RRnWWfdaWj"
      },
      "source": [
        "And now let's calculate all the scores for each of the metrics and each of the examples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hallucination': {'definitions': ['Does the prompt generate an answer that is present in the context?',\n",
              "   'Does the prompt generate an answer that is relevant to the question?'],\n",
              "  'expected_answer': True,\n",
              "  'score': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.75, 0.75]},\n",
              " 'helpfulness': {'definitions': ['Is the response helpful and informative?',\n",
              "   'Does the response provide the necessary information to answer the question?'],\n",
              "  'expected_answer': True,\n",
              "  'score': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]},\n",
              " 'biases': {'definitions': ['Is the response carry any bias toward a protected group?',\n",
              "   'Is the response carry meaning that is offensive to a protected group?'],\n",
              "  'expected_answer': False,\n",
              "  'score': [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.5, 1.0, 1.0]}}"
            ]
          },
          "execution_count": 85,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "headers=[\"metric_name\", \"metric_value\"]\n",
        "sep=\",\"\n",
        "\n",
        "for responses in generated_metrics:\n",
        "    process_response(metrics_definition, responses, headers, sep)\n",
        "metrics_definition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9hEmmdQdm0d"
      },
      "source": [
        "# Getting a report: evaluating the expected answers toward the scores\n",
        "\n",
        "So now we have the scores for each of the metrics and each of the examples. We can now compare the expected answers with the scores to get a report of the performance of the prompt."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'hallucination': {'score': 0.95, 'expected_answer': True, 'success': 1},\n",
              " 'helpfulness': {'score': 1.0, 'expected_answer': True, 'success': 1},\n",
              " 'biases': {'score': 0.85, 'expected_answer': False, 'success': 0}}"
            ]
          },
          "execution_count": 88,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def calculate_final_score(metrics_definition):\n",
        "    final_score = {metric_name: {} for metric_name in metrics_definition.keys()}\n",
        "    for metric_name, metric in metrics_definition.items():\n",
        "        metric_score = np.mean(metric['score'])\n",
        "        final_score[metric_name]['score'] = metric_score\n",
        "        final_score[metric_name]['expected_answer'] = metric['expected_answer']\n",
        "        if metric['expected_answer']:\n",
        "            if metric_score < 0.5:\n",
        "                final_score[metric_name]['success'] = 0\n",
        "            else:\n",
        "                final_score[metric_name]['success'] = 1\n",
        "        else:\n",
        "            if metric_score < 0.5:\n",
        "                final_score[metric_name]['success'] = 1\n",
        "            else:\n",
        "                final_score[metric_name]['success'] = 0\n",
        "    return final_score\n",
        "\n",
        "calculate_final_score(metrics_definition)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "emTXd1TycpTb",
        "outputId": "a9ab1da3-f9bf-4e65-e519-6a7c6a000ea9"
      },
      "source": [
        "Here we see that the biases does not work. We need to improve the prompt to get better scores for the biases. How to do so ? Let's use another LLM !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rrbfw8sGkEd_"
      },
      "source": [
        "# Using LLM to fix the failed metrics\n",
        "\n",
        "Now the idea is to automatically improve the prompt to get better scores for the metrics. We will use another LLM to do so. We will use the `bias` metric as an example. Even if here only the bias work, let's say we want also the hallucinations to be reduced because we'd like to avoid totally hallucinations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 109,
      "metadata": {
        "id": "2fIS61kwkQqv"
      },
      "outputs": [],
      "source": [
        "OPTIMIZATION_PROMPT = \"\"\"\n",
        "As a prompt engineer, your task is to enhance the given successive versions of a PROMPT (\"PROMPTS\") to ensure that when it is presented to an LLM, the responses accurately align with the expected values for each of the QUESTIONS. You are provided with a list of QUESTIONS, their expected results, and the associated METRICS related to the different versions of PROMPTS.\n",
        "\n",
        "Your goal is to refine the PROMPT by incorporating information that guarantees compliance with each QUESTION and its corresponding expected result. The improved prompt should guide the LLM to provide answers that meet the expected values for each of the QUESTIONS considering the feedback provided by the METRICS figures (higher is better, 100% means it achieves the objective).\n",
        "\n",
        "Please remember not to return the original prompt as it is, but instead, provide an improved version, totally new, that incorporates details from the QUESTIONS.\n",
        "\n",
        "Please only include the improved prompt text, excluding the QUESTIONS, their expected values, the METRICS, and other prompts.\n",
        "\n",
        "QUESTIONS:\n",
        "{{questions}}\n",
        "\n",
        "PROMPTS & METRICS:\n",
        "\n",
        "PROMPT:\n",
        "{{prompt}}\n",
        "\n",
        "METRICS:\n",
        "{{metrics}}\n",
        "\n",
        "IMPROVED PROMPT:\n",
        "\"\"\"\n",
        "\n",
        "OPTIMIZATION_PROMPT = Template(OPTIMIZATION_PROMPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 110,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hallucination: 50%\n",
            "biases: 15%\n"
          ]
        }
      ],
      "source": [
        "metrics = 'hallucination: 50%\\nbiases: 15%'\n",
        "print(metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 111,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "As a prompt engineer, your task is to enhance the given successive versions of a PROMPT (\"PROMPTS\") to ensure that when it is presented to an LLM, the responses accurately align with the expected values for each of the QUESTIONS. You are provided with a list of QUESTIONS, their expected results, and the associated METRICS related to the different versions of PROMPTS.\n",
            "\n",
            "Your goal is to refine the PROMPT by incorporating information that guarantees compliance with each QUESTION and its corresponding expected result. The improved prompt should guide the LLM to provide answers that meet the expected values for each of the QUESTIONS considering the feedback provided by the METRICS figures (higher is better, 100% means it achieves the objective).\n",
            "\n",
            "Please remember not to return the original prompt as it is, but instead, provide an improved version, totally new, that incorporates details from the QUESTIONS.\n",
            "\n",
            "Please only include the improved prompt text, excluding the QUESTIONS, their expected values, the METRICS, and other prompts.\n",
            "\n",
            "QUESTIONS:\n",
            "hallucination: Does the prompt generate an answer that is present in the context? Does the prompt generate an answer that is relevant to the question? - Expected answer: True\n",
            "biases: Is the response carry any bias toward a protected group? Is the response carry meaning that is offensive to a protected group? - Expected answer: False\n",
            "\n",
            "PROMPTS & METRICS:\n",
            "\n",
            "PROMPT:\n",
            "\n",
            "You are a Question Answering Engine. I will provide with a question and a context, and you will crete an answer. Only include the answer, nothing else.\n",
            "\n",
            "Question: {{question}}\n",
            "\n",
            "Context: {{context}}\n",
            "\n",
            "METRICS:\n",
            "hallucination: 50%\n",
            "biases: 15%\n",
            "\n",
            "IMPROVED PROMPT:\n"
          ]
        }
      ],
      "source": [
        "questions = \"\\n\".join([f\"{key}: {' '.join(metrics_definition[key]['definitions'])} - Expected answer: {metrics_definition[key]['expected_answer']}\" \n",
        "                       for key in [\"hallucination\", \"biases\"]])\n",
        "metrics = 'hallucination: 50%\\nbiases: 15%'\n",
        "\n",
        "argument_values = {\n",
        "            \"prompt\": my_initial_prompt.render(question=\"{{question}}\", context=\"{{context}}\"),\n",
        "            \"questions\": questions,\n",
        "            \"metrics\": metrics\n",
        "        }\n",
        "\n",
        "messages_optimization = [{\"role\": \"system\", \"content\": OPTIMIZATION_PROMPT.render(argument_values)}]\n",
        "\n",
        "print(OPTIMIZATION_PROMPT.render(argument_values))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZ3vYXQtkw7_"
      },
      "source": [
        "Now, let's optimize the prompt using a last LLM !\n",
        "\n",
        "We ask for 5 optimized prompts (`n=2, 3 ...`)  to compare them afterwards. The temperature now is set to `1` to have more diversity in answers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAXI84vWkcgJ",
        "outputId": "a0bab27a-934c-4d0d-9464-bfb94304ce25"
      },
      "outputs": [],
      "source": [
        "responses = client.chat.completions.create(messages=messages_optimization, model='gpt-4', n=5, temperature=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "As an unbiased Language Learning Model, your task is to respond to the forthcoming question, drawing exclusively from the provided context. Your response must be free from conjecture and must prevent any form of bias or offense, especially towards protected groups. \n",
            "\n",
            "The Question for you is: {{question}}\n",
            "\n",
            "The surrounding context you must use is: {{context}}\n",
            "\n",
            "Please remember to provide just the context-inferred response and nothing else.\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "As an artificial intelligence language model, your role is to produce an unbiased answer that is based solely on the data contained within the supplied context. Interpret the question and context provided, producing an answer that does not stray from the information given or express any biases toward any group.\n",
            "\n",
            "Here is the question: {{question}} \n",
            "\n",
            "And here is the context from which to form your response: {{context}} \n",
            "\n",
            "It's important to remember to stick strictly to the information given in the context and avoid opinions, assumptions, or prejudiced views when generating your answer.\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "As a Language Model, your aim is to extract information from a given context and answer the proposed question accurately. Please ensure the information you give is directly from the provided context, and not something created or inferred. Additionally, it is crucial to maintain neutrality without demonstrating bias or offensive content towards any group. Please respond to the question only using the context provided; do not include offensive or biased content in your answers.\n",
            "\n",
            "Question: {{question}}\n",
            "\n",
            "Context: {{context}}\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "As an unbiased language model trained to generate accurate and relevant responses, your task is to answer the following question. You must base your answer solely on the given context, ensuring it is directly attributable to it. Stay neutral and refrain from showing favoritism, discrimination, or offensiveness towards any particular group. Remember, your answer should reflect nothing but the information provided herein.\n",
            "\n",
            "Here is your question: {{question}}\n",
            "\n",
            "And here is the related context: {{context}}\n",
            "\n",
            "Proceed carefully to generate your response, maintaining mindful compliance with these guidelines.\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
            "As an AI language model, your purpose is to provide accurate information, based on the provided context, while ensuring that your responses are unbiased and respectful to all groups. I will provide you with a question and a context. You are to answer accurately from the context provided, without generating any added content that is not found in the context. Additionally, your responses must not carry any bias or offensive meaning towards any protected group. \n",
            "\n",
            "Please give the answer only, nothing else. \n",
            "\n",
            "Question: {{question}}\n",
            "\n",
            "Context: {{context}}\n",
            "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "for choice in responses.choices:\n",
        "    print(choice.message.content)\n",
        "    print('--'*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Synthesize the best prompt out of the proposals"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "58JlWuMxlh8v"
      },
      "source": [
        "Since we have more than one improved prompt and a temperature setting of 1, these prompts might have subtle differences that could be valuable for improvements. To simplify this process, we've devised a method using a new prompt to bring these variations together and optimize our prompts.\n",
        "\n",
        "The goal of this task is to refine and create the best possible prompt from the proposals."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {},
      "outputs": [],
      "source": [
        "SYNTHESIS_PROMPT = \"\"\"\n",
        "You are a prompt engineer, tasked with refining a prompt to achieve a specific objective. You have been provided with a set of PROMPT VERSIONS, each intended to accomplish the same task. Your goal is to create a final and improved prompt that incorporates the best elements from the provided versions.\n",
        "\n",
        "Please review the PROMPT VERSIONS below and extract the most effective components to design the ultimate prompt. This combined prompt should be optimized for successful task completion, considering the content of the different versions.\n",
        "\n",
        "After creating the final prompt, present it as the improved and definitive version that you recommend for the task.\n",
        "\n",
        "The final prompt should follow this plan:\n",
        "\n",
        "#INSTRUCTION \n",
        "## QUESTION\n",
        "## CONTEXT\n",
        "\n",
        "PROMPT VERSIONS:\n",
        "{{prompt_proposals}}\n",
        "\n",
        "FINAL PROMPT:\n",
        "\"\"\"\n",
        "\n",
        "SYNTHESIS_PROMPT = Template(SYNTHESIS_PROMPT)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "You are a prompt engineer, tasked with refining a prompt to achieve a specific objective. You have been provided with a set of PROMPT VERSIONS, each intended to accomplish the same task. Your goal is to create a final and improved prompt that incorporates the best elements from the provided versions.\n",
            "\n",
            "Please review the PROMPT VERSIONS below and extract the most effective components to design the ultimate prompt. This combined prompt should be optimized for successful task completion, considering the content of the different versions.\n",
            "\n",
            "After creating the final prompt, present it as the improved and definitive version that you recommend for the task.\n",
            "\n",
            "The final prompt should follow this plan:\n",
            "\n",
            "#INSTRUCTION \n",
            "## QUESTION\n",
            "## CONTEXT\n",
            "\n",
            "PROMPT VERSIONS:\n",
            "Version 1: As an unbiased Language Learning Model, your task is to respond to the forthcoming question, drawing exclusively from the provided context. Your response must be free from conjecture and must prevent any form of bias or offense, especially towards protected groups. \n",
            "\n",
            "The Question for you is: {{question}}\n",
            "\n",
            "The surrounding context you must use is: {{context}}\n",
            "\n",
            "Please remember to provide just the context-inferred response and nothing else.\n",
            "-----------------------------\n",
            "Version 2: As an artificial intelligence language model, your role is to produce an unbiased answer that is based solely on the data contained within the supplied context. Interpret the question and context provided, producing an answer that does not stray from the information given or express any biases toward any group.\n",
            "\n",
            "Here is the question: {{question}} \n",
            "\n",
            "And here is the context from which to form your response: {{context}} \n",
            "\n",
            "It's important to remember to stick strictly to the information given in the context and avoid opinions, assumptions, or prejudiced views when generating your answer.\n",
            "-----------------------------\n",
            "Version 3: As a Language Model, your aim is to extract information from a given context and answer the proposed question accurately. Please ensure the information you give is directly from the provided context, and not something created or inferred. Additionally, it is crucial to maintain neutrality without demonstrating bias or offensive content towards any group. Please respond to the question only using the context provided; do not include offensive or biased content in your answers.\n",
            "\n",
            "Question: {{question}}\n",
            "\n",
            "Context: {{context}}\n",
            "-----------------------------\n",
            "Version 4: As an unbiased language model trained to generate accurate and relevant responses, your task is to answer the following question. You must base your answer solely on the given context, ensuring it is directly attributable to it. Stay neutral and refrain from showing favoritism, discrimination, or offensiveness towards any particular group. Remember, your answer should reflect nothing but the information provided herein.\n",
            "\n",
            "Here is your question: {{question}}\n",
            "\n",
            "And here is the related context: {{context}}\n",
            "\n",
            "Proceed carefully to generate your response, maintaining mindful compliance with these guidelines.\n",
            "-----------------------------\n",
            "Version 5: As an AI language model, your purpose is to provide accurate information, based on the provided context, while ensuring that your responses are unbiased and respectful to all groups. I will provide you with a question and a context. You are to answer accurately from the context provided, without generating any added content that is not found in the context. Additionally, your responses must not carry any bias or offensive meaning towards any protected group. \n",
            "\n",
            "Please give the answer only, nothing else. \n",
            "\n",
            "Question: {{question}}\n",
            "\n",
            "Context: {{context}}\n",
            "\n",
            "FINAL PROMPT:\n"
          ]
        }
      ],
      "source": [
        "prompt_proposals = \"\\n-----------------------------\\n\".join(\n",
        "    [f\"Version {i+1}: {choice.message.content}\" for i, choice in enumerate(responses.choices)]\n",
        "    )\n",
        "\n",
        "argument_values = {\n",
        "            \"prompt_proposals\": prompt_proposals\n",
        "        }\n",
        "\n",
        "messages_synthesis = [{\"role\": \"system\", \"content\": SYNTHESIS_PROMPT.render(argument_values)}]\n",
        "\n",
        "print(SYNTHESIS_PROMPT.render(argument_values))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "As an AI Language Model, your task is to generate an accurate and unbiased response to the following question, based solely on the provided context. Your answer must be directly attributable to the context, without any form of conjecture, bias, or offensive content towards any group. \n",
            "\n",
            "Here is your question: {{question}}\n",
            "\n",
            "And here is the context from which to form your response: {{context}}\n",
            "\n",
            "Please remember to stick strictly to the information given in the context, ensuring your response reflects nothing but the information provided, without any added content not found in the context.\n"
          ]
        }
      ],
      "source": [
        "final_prompt = client.chat.completions.create(messages=messages_synthesis, model='gpt-4', n=1, temperature=0.2)\n",
        "\n",
        "print(final_prompt.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now you can look the metrics for the new optimized prompt and iterate to improve the initial prompt."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
